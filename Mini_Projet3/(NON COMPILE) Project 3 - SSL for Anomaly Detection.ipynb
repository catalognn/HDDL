{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e83c39-a69d-46d5-87a5-44454240d924",
   "metadata": {},
   "source": [
    "# Projet 3 - SSL pour la détection d'Anomalie - Groupe 12\n",
    "Groupe 12 : Lise Catalogna, Julien Richaume, Romain Cargnello, Romain Laurié."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "import zipfile\n",
    "import itertools\n",
    "from itertools import product\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "   \n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bf58a-d903-493a-858e-5d588d877e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#POUR IMPORTER DES PAQUETS DANS L'ENVIRONNEMENT IPYKERNEL JUPYTER NOTEBOOK\n",
    "#import sys\n",
    "#!\"{sys.executable}\" -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357122c-660a-4d6e-bb18-f74a362d76d4",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "\n",
    "On doit charger les datasets depuis les sites web, qui sont dans des dossiers compréssés. Ainsi, on crée deux fonctions téléchargeants directement les fichiers et en les décompressant, une fonction pour chaque type de compression, zip et tar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(url, output_dir, zip_name=\"images.zip\"):\n",
    "    \"\"\"\n",
    "    Télécharge un fichier zip contenant des images et le décompresse dans un dossier.\n",
    "    \n",
    "    :param url: URL du fichier zip\n",
    "    :param output_dir: Répertoire où les images seront décompressées\n",
    "    :param zip_name: Nom temporaire pour le fichier zip téléchargé\n",
    "    \"\"\"\n",
    "    # Download the zipfile on the wanted website\n",
    "    print(f\"Téléchargement du fichier depuis {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(zip_name, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(\"Téléchargement terminé.\")\n",
    "    else:\n",
    "        print(\"Erreur lors du téléchargement.\")\n",
    "        return\n",
    "\n",
    "    # Create the output repo if necessary\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Unzip the file\n",
    "    print(f\"Décompression dans {output_dir}...\")\n",
    "    with zipfile.ZipFile(zip_name, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "    print(\"Décompression terminée.\")\n",
    "\n",
    "    # Delete the temporary zip file\n",
    "    os.remove(zip_name)\n",
    "    print(\"Fichier zip temporaire supprimé.\")\n",
    "\n",
    "\n",
    "def download_and_extract_tar(url, output_dir, tar_name=\"images.tar.gz\"):\n",
    "    \"\"\"\n",
    "    Télécharge un fichier TAR contenant des images et le décompresse dans un dossier.\n",
    "    \n",
    "    :param url: URL du fichier TAR\n",
    "    :param output_dir: Répertoire où les images seront décompressées\n",
    "    :param tar_name: Nom temporaire pour le fichier TAR téléchargé\n",
    "    \"\"\"\n",
    "    # Download the TAR file on the wanted website\n",
    "    print(f\"Téléchargement du fichier depuis {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(tar_name, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(\"Téléchargement terminé.\")\n",
    "    else:\n",
    "        print(\"Erreur lors du téléchargement.\")\n",
    "        return\n",
    "\n",
    "    # Create the output repo if necessary\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Untar the file\n",
    "    print(f\"Décompression dans {output_dir}...\")\n",
    "    if tarfile.is_tarfile(tar_name):\n",
    "        with tarfile.open(tar_name, \"r:*\") as tar_ref:\n",
    "            tar_ref.extractall(output_dir)\n",
    "        print(\"Décompression terminée.\")\n",
    "    else:\n",
    "        print(f\"Le fichier {tar_name} n'est pas un fichier TAR valide.\")\n",
    "        return\n",
    "\n",
    "    # Delete the temporary TAR file\n",
    "    os.remove(tar_name)\n",
    "    print(\"Fichier TAR temporaire supprimé.\")\n",
    "\n",
    "## Definir le répertoire de sortie   \n",
    "output_dir = r\"/home/rlaurie/Bureau/5A/HDDL/projet3/placeholder\"\n",
    "#output_dir = r\"/home/catalogn/Documents/5A/HDDL/projets/projet3\"\n",
    "    \n",
    "url = \"https://zenodo.org/records/10459003/files/engine_wiring.zip?download=1\" \n",
    "#download_and_unzip(url, output_dir)\n",
    "\n",
    "url1 = \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937454-1629951595/capsule.tar.xz\"\n",
    "#download_and_extract_tar(url1, output_dir)\n",
    "\n",
    "url2 = \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937545-1629951845/hazelnut.tar.xz\"\n",
    "#download_and_extract_tar(url2, output_dir)\n",
    "\n",
    "url3 = \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938134-1629953256/toothbrush.tar.xz\"\n",
    "#download_and_extract_tar(url3, output_dir)\n",
    "\n",
    "url4 = \"https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420937370-1629951468/bottle.tar.xz\"\n",
    "#download_and_extract_tar(url4, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49563d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On vérifie qu'on a bien chargé nos images en en affichant une\n",
    "image = mpimg.imread('bottle/train/good/000.png')\n",
    "image2 = mpimg.imread('capsule/train/good/000.png')\n",
    "image3 = mpimg.imread('hazelnut/train/good/000.png')\n",
    "plt.imshow(image3)\n",
    "plt.axis('off')  # Optionnel : désactiver les axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ca2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#On fait la même chose en affichant une image de chaque jeu de données\n",
    "images = [\n",
    "    mpimg.imread('bottle/train/good/000.png'),\n",
    "    mpimg.imread('capsule/train/good/000.png'),\n",
    "    mpimg.imread('hazelnut/train/good/000.png'),\n",
    "    mpimg.imread('toothbrush/train/good/000.png'),\n",
    "    mpimg.imread('engine_wiring/train/good/0000.png')\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    plt.subplot(1, len(images), i + 1)  # 1 ligne, n colonnes, position i+1\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30296db1-34fc-4b3b-b4c8-42dcb16b3e4c",
   "metadata": {},
   "source": [
    "Maintenant que nos images sont bien chargés, on doit s'assurer qu'elles sont bien dans un même format. Or on a remarqué que lors du chargement il y a des différences, ainsi nous uniformisons tous cela grâce à la fonctions suivante.\n",
    "\n",
    "Cette fonction permet à la fois de déplacer et de renommer les images, en effet on a remarqué qu'engine_wiring avait un nom différents avec xxxx.png au lieu de xxx.png, ainsi on a la renomme. De plus, pour simplifier l'accès aux données plus tard, on regroupe toutes les anomalies dans un dossier bad, ce qui permet d'uniformiser nos fonctions pour la suite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f850da-bfe1-4fb1-8eb7-1c87d1c18a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "def move_and_rename_images(src_dir, dest_dir):\n",
    "    \"\"\"\n",
    "    Déplace et renomme les images des sous-dossiers dans un seul dossier cible.\n",
    "    \n",
    "    :param src_dir: Chemin du dossier contenant les sous-dossiers\n",
    "    :param dest_dir: Chemin du dossier cible\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)  # Crée le dossier cible s'il n'existe pas\n",
    "\n",
    "    counter = 1  # Compteur pour différencier les fichiers\n",
    "    for root, _, files in os.walk(src_dir):  # Parcourt récursivement les sous-dossiers\n",
    "        for file in files:\n",
    "            if file.endswith(\".png\"):  # Vérifie que c'est une image .png\n",
    "                # Chemin complet de l'image source\n",
    "                src_path = os.path.join(root, file)\n",
    "                \n",
    "                # Nouveau nom avec compteur\n",
    "                new_name = f\"{counter}.png\"\n",
    "                dest_path = os.path.join(dest_dir, new_name)\n",
    "                \n",
    "                # Déplace et renomme l'image\n",
    "                shutil.move(src_path, dest_path)\n",
    "                print(f\"Moved: {src_path} -> {dest_path}\")\n",
    "                \n",
    "                counter += 1  # Incrémente le compteur\n",
    "\n",
    "src_directory = \"engine_wiring/test/bad\"\n",
    "dest_directory = \"engine_wiring/test/bad\" \n",
    "#move_and_rename_images(src_directory, dest_directory)\n",
    "\n",
    "src_directory = \"bottle/test/bad\"\n",
    "dest_directory = \"bottle/test/bad\" \n",
    "#move_and_rename_images(src_directory, dest_directory)\n",
    "\n",
    "src_directory = \"capsule/test/bad\"\n",
    "dest_directory = \"capsule/test/bad\" \n",
    "#move_and_rename_images(src_directory, dest_directory)\n",
    "\n",
    "src_directory = \"hazelnut/test/bad\"\n",
    "dest_directory = \"hazelnut/test/bad\" \n",
    "#move_and_rename_images(src_directory, dest_directory)\n",
    "\n",
    "src_directory = \"toothbrush/test/bad\"\n",
    "dest_directory = \"toothbrush/test/bad\" \n",
    "#move_and_rename_images(src_directory, dest_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf01b7-197d-48ba-952a-4cdbd1b9c679",
   "metadata": {},
   "source": [
    "Maintenant que nos données sont uniformisées, on peut créer les Tenseurs et les DataLoaders encodant nos images.\n",
    "\n",
    "Après plusieurs tests, nous avons remarqués, que normaliser nos données faisait baisser les précisions de nos modèles, ainsi nous avons décidé de ne pas les normaliser. Cependant nous redimmensions toutes. Pour cela, nous avons tout d'abord pris la taille standardisée (224,224), mais nous avons décidé de plutôt opter pour du (400,400), qui est la taille de la plus petit image de notre dataset. En effet, cerrtaines anomalies, par exemple pour le jeu de données \"toothbrush\" sont assez inifimes, ainsi en baissant encore plus la dimension de nos images nous avions peur que notre modèle n'arrive pas à distinguer les anomalies, étant devenues trop petites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8b71f-6def-454c-850e-514225dcb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = ['bottle', 'hazelnut', 'toothbrush', 'engine_wiring', 'capsule']\n",
    "im_train = []\n",
    "im_test = [[] for _ in range(len(data_dir))]\n",
    "i=0\n",
    "for data in data_dir:\n",
    "    print(\"######\")\n",
    "    print(data)\n",
    "    # Définir les transformations des images (normalisation, redimensionnement, etc.)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((400, 400)),  # Redimensionner les images\n",
    "        transforms.ToTensor(),  # Convertir les images en tenseurs\n",
    "        #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalisation standard pour les modèles pré-entraînés\n",
    "    ])\n",
    "    \n",
    "    # Charger le dataset en spécifiant le dossier train ou test\n",
    "    train_dataset = datasets.ImageFolder(root=f\"{data}/train\", transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=f\"{data}/test\", transform=transform)\n",
    "\n",
    "    # Créer des variables distinctes pour les DataLoaders\n",
    "    exec(f\"{data}_train_dataset = train_dataset\")\n",
    "    exec(f\"{data}_test_dataset = test_dataset\")\n",
    "    \n",
    "    # Créer les DataLoaders\n",
    "    batch_size = 32\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Créer des variables distinctes pour les DataLoaders\n",
    "    exec(f\"{data}_train_loader = train_loader\")\n",
    "    exec(f\"{data}_test_loader = test_loader\")\n",
    "\n",
    "    \n",
    "    # Vérifier le nombre d'images par dataset\n",
    "    print(f\"Nombre d'images pour le train : {len(train_dataset)}\")\n",
    "    im_train.append(len(train_dataset))\n",
    "    print(f\"Nombre d'images pour le test : {len(test_dataset)}\")\n",
    "    \n",
    "    # Classes\n",
    "    print(f\"Classes détectées dans train : {train_dataset.classes}\")\n",
    "    print(f\"Classes détectées dans test : {test_dataset.classes}\")\n",
    "\n",
    "    # Récupérer les indices des classes pour chaque image\n",
    "    class_counts = Counter(test_dataset.targets)\n",
    "    \n",
    "    # Obtenir les noms des classes\n",
    "    class_names = test_dataset.classes\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    for class_idx, count in class_counts.items():\n",
    "        print(f\"Classe '{class_names[class_idx]}': {count} images\")\n",
    "        im_test[i].append(count)\n",
    "    i+=1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd289c3-f648-475e-8dea-0371675d0ab6",
   "metadata": {},
   "source": [
    "Maintenant que nos DataLoaders sont crées, on regarde leur composition ci-dessus, mais aussi avec des histogrammes ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd866643-ab1f-4184-9d5e-c016130bb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des catégories et des données d'entraînement/test\n",
    "\n",
    "categories = ['bottle', 'hazelnut', 'toothbrush', 'engine_wiring', 'capsule']\n",
    "\n",
    "# Indices des catégories pour positionner les barres\n",
    "indices = np.arange(len(categories))\n",
    "\n",
    "# Largeur des barres\n",
    "bar_width = 0.35\n",
    "print(im_test)\n",
    "\n",
    "# Calcul des segments pour chaque classe\n",
    "test_class_1 = [counts[0] for counts in im_test]\n",
    "test_class_2 = [counts[1] for counts in im_test]\n",
    "\n",
    "# Création de l'histogramme\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(indices, im_train, bar_width, label='Train', color='mediumorchid', alpha=0.7)\n",
    "plt.bar(indices + bar_width, test_class_1, bar_width, label='Test - Bad', color='tomato', alpha=0.7)\n",
    "plt.bar(indices + bar_width, test_class_2, bar_width, bottom=test_class_1, label='Test - Good', color='forestgreen', alpha=0.7)\n",
    "\n",
    "# Ajout des étiquettes et des titres\n",
    "plt.xlabel('Catégories', fontsize=12)\n",
    "plt.ylabel('Nombre d\\'images', fontsize=12)\n",
    "plt.title('Répartition des images entre train et test par catégorie', fontsize=14)\n",
    "plt.xticks(indices + bar_width / 2, categories, rotation=15, fontsize=10)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1db3a2-2f28-4838-9fe8-5bb76e81c580",
   "metadata": {},
   "source": [
    "On remarque plusieurs choses : \n",
    "- Pour \"engine_wiring\" il y a beaucoup plus d'images dans le test que dans le train ce qui pourrait peut être poser problème plus tard.\n",
    "- Pour \"toothbursh\" il y a, aussi, peu de données tout court, que ça soit pour le train comme pour le test, ainsi notre modèle pourra peut être surapprendre sur nos images, ce qui pourra être un problème. De plus, la répartition est d'environ 60-40, ce qui est inférieur à ce qu'on a habituellement, ce qui pourrait peut être également poser problème plus tard.\n",
    "- Pour le reste il y a plus de données d'entraîenements que de tests, et plus d'anomalies que de bonnes images dans le jeu de données de tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd2f25-98a0-49b2-bd77-2c9e121adb2c",
   "metadata": {},
   "source": [
    "## Définition des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8487a-33ab-4494-9007-81657b897ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, latent_dim=128):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 8x8 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, latent_dim, kernel_size=4, stride=2, padding=1),  # 4x4 -> 2x2\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=128, out_channels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 256, kernel_size=4, stride=2, padding=1),  # 2x2 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 4x4 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 8x8 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1),  # 16x16 -> 32x32\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13287cd3-a239-436a-be5c-e1b2b8c9730d",
   "metadata": {},
   "source": [
    "On définit désormais la fonction qui servira pour entraîner nos modèles, elle servira aussi à afficher les courbes de pertes, pour vérifier le bon apprentissage de nos modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb7ee8-0a1d-4e4c-899f-807533e31e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssl_model(model, \n",
    "                    train_loader, \n",
    "                    test_loader, \n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    device='cuda',\n",
    "                    epochs=5,\n",
    "                    show_plot=False):\n",
    "    # Initialiser les listes pour stocker les pertes\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(images)\n",
    "            loss = criterion(output, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)  # Stocker la perte d'entraînement\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, _ in test_loader:\n",
    "                images = images.to(device)\n",
    "                output, _ = model(images)\n",
    "                val_loss = criterion(output, images)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        val_losses.append(avg_val_loss)  # Stocker la perte de validation\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    if show_plot:\n",
    "        # Tracer les courbes de pertes\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "        plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
    "        plt.xlabel('Époque', fontsize=12)\n",
    "        plt.ylabel('Loss', fontsize=12)\n",
    "        plt.title('Courbes de Training et Validation Loss', fontsize=14)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return model.encoder, avg_val_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f786c-3e16-4f45-8999-5dbbdba62be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la recpnstruction d'images aléatoires dans le dataset de test\n",
    "def visualize_reconstructions(model, data_loader, device, num_images=5):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = list(data_loader.dataset)   # Pour convertir les DataLoader en un échantillon aléatoire d'image\n",
    "    random_indices = random.sample(range(len(dataset)), num_images) # Sélection aléatoire des images `num_images` du jeu de données\n",
    "    random_images = [dataset[i][0] for i in random_indices]  # Extraire uniquement les images, en ignorant les étiquettes\n",
    "\n",
    "    images = torch.stack(random_images)   # Empiler les images dans un lot\n",
    "    print(device)\n",
    "    images = images.to(device)\n",
    "    \n",
    "    # On passe les images dans le modèle mis en argument\n",
    "    with torch.no_grad():\n",
    "        reconstructed_images, perturbed_images = model(images)\n",
    "    \n",
    "    # On remet les images sur le CPU pour la visualization\n",
    "    images = images.cpu()\n",
    "    reconstructed_images = reconstructed_images.cpu()\n",
    "    perturbed_images = perturbed_images.cpu()\n",
    "    \n",
    "    # Afficher l'image reçue par le modèle, la vérité terrain et l'image reconstruite\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(10, num_images * 4))\n",
    "    for i in range(num_images):\n",
    "        # Grayscale input\n",
    "        axes[i, 0].imshow(perturbed_images[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "        axes[i, 0].set_title(\"Input du modèle\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Ground truth (original RGB image)\n",
    "        axes[i, 1].imshow(images[i].permute(1, 2, 0))\n",
    "        axes[i, 1].set_title(\"Vérité Terrain\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Colorized output from the model\n",
    "        axes[i, 2].imshow(reconstructed_images[i].permute(1, 2, 0))\n",
    "        axes[i, 2].set_title(\"Image Reconstruite\")\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a310380-6dd6-4577-93eb-e496c368f15e",
   "metadata": {},
   "source": [
    "## Méthode 1 : COLORIZATION\n",
    "\n",
    "On commence avec le premier modèle : Colorization.\n",
    "\n",
    "Le modèle de colorisation apprend à convertir des images en niveaux de gris en images colorées. Cette tâche impose au modèle de capturer des caractéristiques de haut niveau dans les images, ce qui est utile pour détecter des différences subtiles ou des anomalies. Plus précisement, on l'a choisi car dans le cas de la détection d'anomalie, une mauvaise reconstruction des couleurs indique que le modèle ne reconnaît pas certaines caractéristiques, ce qui est un bon indicateur d'une anomalie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5847fa-d58f-4372-b15f-460c70bc9955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationModel(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, in_channels=1)  # Input grayscale\n",
    "        self.decoder = Decoder(latent_dim=latent_dim, out_channels=3)  # Predict RGB\n",
    "\n",
    "    def forward(self, x):\n",
    "        grayscale_x = transforms.Grayscale()(x)  # Convert RGB to Grayscale\n",
    "        z = self.encoder(grayscale_x)\n",
    "        return self.decoder(z), grayscale_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db7a2e-5928-44c3-a737-534269f586ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "colorization_model = ColorizationModel(latent_dim=128)\n",
    "colorization_encoder, val, colorization_model = train_ssl_model(colorization_model, \n",
    "                                                                hazelnut_train_loader, \n",
    "                                                                hazelnut_test_loader, \n",
    "                                                                criterion=nn.MSELoss(), \n",
    "                                                                optimizer=optim.Adam(colorization_model.parameters(), lr=0.001),\n",
    "                                                                device=device,\n",
    "                                                                epochs=20\n",
    "                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d12fb6-c5d6-4e49-b691-6cc8f853e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la colorisation sur des images test aléatoires\n",
    "visualize_reconstructions(colorization_model, hazelnut_test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d2e39-c0ae-4c32-ad83-f184c5e05339",
   "metadata": {},
   "source": [
    "On remarque qu'avec un modèle pas optimisé et peu d'époques on arrive bien à reconstruire les anomalies, surtout les trous dans la noisette. Cependant les images sont tout de même assez floues, ce qui pose problème pour détecter des coupures ou des rayures sur la noisette."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7e51c-3a15-4f22-8dc5-64fd4f5b22e8",
   "metadata": {},
   "source": [
    "## Méthode 2 : INPAINTING\n",
    "\n",
    "Le 2ème modèle utilisé est l'inpainting. C'est une tâche qui consiste à reconstruire des parties manquantes dans une image, qui sont masquées par un carré noir (masque) intégré aux images dans l'entraînement. On a choisi ce modèle est choisi pour sa capacité à apprendre des représentations visuelles en explorant les relations contextuelles entre différentes régions de l'image.\n",
    "\n",
    "Si un modèle est entraîné à reconstruire des images partielles (avec des masques), les anomalies deviendront plus visibles par des reconstructions moins précises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9261a1-601a-4149-8da9-3fe749be36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InpaintingModel(nn.Module):\n",
    "    def __init__(self, latent_dim=128, mask_size=8):\n",
    "        super(InpaintingModel, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim)\n",
    "        self.mask_size = mask_size\n",
    "    \n",
    "    #forward method\n",
    "    def forward(self, x):\n",
    "        x_masked = self.apply_mask(x)\n",
    "        z = self.encoder(x_masked)\n",
    "        return self.decoder(z), x_masked\n",
    "\n",
    "    #mask method\n",
    "    def apply_mask(self, x):\n",
    "        masked_x = x.clone()\n",
    "\n",
    "        for i in range(masked_x.size(0)):\n",
    "            ul_x = np.random.randint(0, x.size(2) - self.mask_size + 1) #Randomly sample the x coordinate of the upper left corner\n",
    "            ul_y = np.random.randint(0, x.size(3) - self.mask_size + 1) #Randomly sample the y coordinate of the upper left corner\n",
    "            masked_x[i, :, ul_x:ul_x+self.mask_size, ul_y:ul_y+self.mask_size] = 0\n",
    "\n",
    "        return masked_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f6ebd-cb2d-42b7-aa0c-0896ebfe90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainting_model = InpaintingModel(mask_size=36) \n",
    "inpainting_encoder, val, inpainting_model = train_ssl_model(inpainting_model,\n",
    "                                                            capsule_train_loader, \n",
    "                                                            capsule_test_loader, \n",
    "                                                            criterion=nn.MSELoss(), \n",
    "                                                            optimizer=optim.Adam(inpainting_model.parameters(), lr=0.001),\n",
    "                                                            device=device,\n",
    "                                                            epochs=20\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac12fb-b0f0-4863-a6bd-b630993876db",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reconstructions(inpainting_model, capsule_test_loader, device=device, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31203d-e37e-4044-9cf4-d479e07db366",
   "metadata": {},
   "source": [
    "On remarque plus ou moins la même chose qu'avec le modèle de colorization et le dataset des noisettes. On a du mal à bien reconstruire le chiffre sur la capsule et on arrive à peu près bien à reconstruire les capsules, et les masques semblent bien perturbées un peu l'images ce qui force le modèle à se concentrer sur sa reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7ebbc-b3a6-43be-9c9c-6c3c04019ab6",
   "metadata": {},
   "source": [
    "## Méthode 3 : Masked Autoencoder\n",
    "\n",
    "Pour finir, on a choisir de partir avec un Masked AutoEncoder. Les MAEs masquent une proportion significative des pixels d'entrée et apprennent à reconstruire ces pixels masqués. On a choisi ce modèle, car cela le pousse à comprendre globalement les structures et les textures des images. Ainsi la reconstruction des régions masquées d'une image anormale est souvent moins précise, ce qui fait des MAEs un choix adapté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57af0feb-2909-41d0-bb52-89728e2f41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoderModel(nn.Module):\n",
    "    def __init__(self, latent_dim=128, mask_ratio=1/16):\n",
    "        super(MaskedAutoencoderModel, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim)\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_masked = self.apply_mask(x)\n",
    "        z = self.encoder(x_masked)\n",
    "        return self.decoder(z), x_masked\n",
    "    \n",
    "    def apply_mask(self, x):\n",
    "        x_masked = x.clone()\n",
    "        mask = torch.rand_like(x[:, 0, :, :]) < self.mask_ratio\n",
    "        mask = mask.unsqueeze(1).repeat(1, x.size(1), 1, 1)\n",
    "        x_masked[mask] = 0\n",
    "        return x_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0f489-9e14-4a68-99cd-8d3ea26438bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_model = MaskedAutoencoderModel(latent_dim=128, mask_ratio=2/16)\n",
    "mae_encoder, val, mae_model = train_ssl_model(mae_model,\n",
    "                                              engine_wiring_train_loader, \n",
    "                                              engine_wiring_test_loader, \n",
    "                                              criterion=nn.MSELoss(), \n",
    "                                              optimizer=optim.Adam(mae_model.parameters(), lr=0.001),\n",
    "                                              device=device,\n",
    "                                              epochs=20\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07725c-7eed-437b-890d-6d302e35074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reconstructions(mae_model, engine_wiring_test_loader, device=device, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558cb2d0-2b4b-4137-98e9-41761d090a3e",
   "metadata": {},
   "source": [
    "De même qu'avant, on reconnait bien les câbles mais on ne distingue pas parfaitement les couleurs notamment le vert et le bleu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a5c4c-7eb5-45a9-9f39-9194e172e437",
   "metadata": {},
   "source": [
    "## Fine Tunning\n",
    "\n",
    "Maintenant que nos modèles sont définis, on voit que les résultats sont largement améliorables. Pour cela, on a décider de finetuner nos hyperparamètres en dehors de ce notebook sur le serveur de calcul du GMM, ici vous pourrez retrouver les fonctions de GridSearch que nous avons utilisé pour cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d4ada-f8a0-4847-bbe4-0907df3378bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On a modifié la fonction d'entraînement des modèles pour qu'elles n'affiche plus les courbes\n",
    "#Cependant elle intégre désormais le fais d'écrire les résultats dans un fichier excel toutes les 5 époques.\n",
    "\n",
    "def train_ssl_model_csv(model, \n",
    "                    train_loader, \n",
    "                    test_loader, \n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    device='cuda',\n",
    "                    epochs=5,\n",
    "                    model_name=\"Model\",\n",
    "                    dataset_name=\"Dataset\",\n",
    "                    param_dict=None,\n",
    "                    save_path=\"results.csv\"):\n",
    "    # Initialiser les listes pour stocker les pertes\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(images)\n",
    "            loss = criterion(output, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)  # Stocker la perte d'entraînement\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, _ in test_loader:\n",
    "                images = images.to(device)\n",
    "                output, _ = model(images)\n",
    "                val_loss = criterion(output, images)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        val_losses.append(avg_val_loss)  # Stocker la perte de validation\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Sauvegarde dans un fichier CSV toutes les 5 époques\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            with open(save_path, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\n",
    "                    model_name, dataset_name, epoch + 1, \n",
    "                    param_dict, avg_train_loss, avg_val_loss\n",
    "                ])\n",
    "\n",
    "    return model.encoder, avg_val_loss\n",
    "\n",
    "\n",
    "# On utilise cette fonction pour pouvoir optimiser les hyperparamètres :\n",
    "# de mask_ratio pour Inpainting \n",
    "# de mask_size pour  MAE\n",
    "\n",
    "def get_model_specific_param_grid(model_name, param_grid):\n",
    "    param_grid = param_grid.copy()  # Créer une copie pour éviter d'affecter l'original\n",
    "    if model_name != \"MaskedAutoencoder\":\n",
    "        param_grid.pop('mask_ratio', None)  # Retirer 'mask_ratio' si non pertinent\n",
    "    if model_name != \"Inpainting\":\n",
    "        param_grid.pop('mask_size', None)  # Retirer 'mask_size' si non pertinent\n",
    "    return param_grid\n",
    "\n",
    "\n",
    "def grid_search_ssl(models, datasets, param_grid, criterion, device='cuda', epochs=5, save_path=\"results.csv\"):\n",
    "    \"\"\"\n",
    "    Recherche en GridSearch de modèles d'apprentissage auto-supervisé sur plusieurs ensembles de données.\n",
    "\n",
    "    Args :\n",
    "        models (dict) : Dictionnaire de modèles avec leurs noms comme clés.\n",
    "        datasets (dict) : Dictionnaire avec les noms des ensembles de données comme clés et les tuples (train_loader, test_loader) comme valeurs.\n",
    "        param_grid (dict) : Grille d'hyperparamètres (les clés sont des hyperparamètres, les valeurs sont des listes de valeurs possibles).\n",
    "        criterion : Fonction de perte.\n",
    "        device (str) : Dispositif à utiliser ('cuda' ou 'cpu').\n",
    "        epochs (int) : Nombre d'époques à entraîner.\n",
    "        save_path (str) : Chemin d'accès au fichier pour l'enregistrement des résultats.\n",
    "\n",
    "    Retourne :\n",
    "        results (dict) : Meilleurs paramètres et perte de validation correspondante pour chaque modèle et ensemble de données.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Générer l'en-tête du fichier CSV\n",
    "    with open(save_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Model\", \"Dataset\", \"Epoch\", \"Parameters\", \"Train Loss\", \"Validation Loss\"])\n",
    "\n",
    "    # Générer toutes les combinaisons d'hyperparamètres\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "    for model_name, model_class in models.items():\n",
    "        print(f\"==== Grid Search for Model: {model_name} ====\")\n",
    "        results[model_name] = {}\n",
    "\n",
    "        for dataset_name, (train_loader, test_loader) in datasets.items():\n",
    "            print(f\"  Dataset: {dataset_name}\")\n",
    "            best_loss = float('inf')\n",
    "            best_params = None\n",
    "\n",
    "            # Obtenir le param_grid spécifique au modèle\n",
    "            param_grid = get_model_specific_param_grid(model_name, param_grid)\n",
    "\n",
    "            # Générer toutes les combinaisons de paramètres pertinentes\n",
    "            param_combinations = list(itertools.product(*param_grid.values()))\n",
    "\n",
    "            for params in param_combinations:                \n",
    "                # Créer le modèle avec les hyperparamètres actuels\n",
    "                param_dict = dict(zip(param_grid.keys(), params))\n",
    "                model_params = {k: v for k, v in param_dict.items() if k not in ['lr']}\n",
    "                if model_name != \"MaskedAutoencoder\" :\n",
    "                    model_params = {k: v for k, v in model_params.items() if k not in ['mask_ratio']}\n",
    "                if model_name != \"Inpainting\" :\n",
    "                    model_params = {k: v for k, v in model_params.items() if k not in ['mask_size']}\n",
    "                # Afficher la combinaison de paramètres testée\n",
    "                print(f\"Testing parameters: {param_dict}\")\n",
    "                model = model_class(**model_params).to(device)\n",
    "\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=param_dict['lr'])\n",
    "\n",
    "                # Entraîner le modèle\n",
    "                trained_encoder, val_loss = train_ssl_model_csv(\n",
    "                    model,\n",
    "                    train_loader,\n",
    "                    test_loader,\n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    device=device,\n",
    "                    epochs=epochs,\n",
    "                    model_name=model_name,\n",
    "                    dataset_name=dataset_name,\n",
    "                    param_dict=param_dict,\n",
    "                    save_path=save_path\n",
    "                )\n",
    "\n",
    "                # Vérifier si la configuration actuelle est la meilleure\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_params = param_dict\n",
    "\n",
    "            # Stocker les meilleurs paramètres et les pertes\n",
    "                results[model_name][dataset_name] = {\n",
    "                'best_params': best_params,\n",
    "                'best_val_loss': best_loss\n",
    "            }\n",
    "\n",
    "            # Ajouter les meilleurs résultats au CSV\n",
    "            with open(save_path, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\n",
    "                    model_name, dataset_name, \"Best\", best_params, \"-\", best_loss\n",
    "                ])\n",
    "\n",
    "            print(f\"    Best Params: {best_params}, Best Val Loss: {best_loss:.4f}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b2fbf-45ae-4ae4-9603-25202b266320",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'MaskedAutoencoder': lambda **params: MaskedAutoencoderModel(**params),\n",
    "    'Colorization': lambda **params: ColorizationModel(**params),\n",
    "    'Inpainting': lambda **params: InpaintingModel(**params),\n",
    "}\n",
    "\n",
    "datasets = {\n",
    "    'bottle': (bottle_train_loader, bottle_test_loader),\n",
    "    'hazelnut': (hazelnut_train_loader, hazelnut_test_loader),\n",
    "    'toothbrush': (toothbrush_train_loader, toothbrush_test_loader),\n",
    "    'engine_wiring': (engine_wiring_train_loader, engine_wiring_test_loader),\n",
    "    'capsule': (capsule_train_loader, capsule_test_loader),\n",
    "}\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'lr': [0.01, 0.005, 0.001, 0.0001],\n",
    "    'latent_dim': [64, 128, 256],\n",
    "    'mask_size': [16, 32, 64],  # Pour InpaintingModel uniquement\n",
    "    'mask_ratio': [1/16, 1/8, 1/4],  # Pour MaskedAutoencoderModel uniquement\n",
    "}\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#results = grid_search_ssl(models, datasets, param_grid, criterion, device, epochs=2)\n",
    "#print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2d80c-32f5-4428-b5e0-7f25187c4aa1",
   "metadata": {},
   "source": [
    "Après le code pour sélectionner nos hyperparamètres, on les extrait du fichier excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c3a12-3989-4114-9b05-4711a02c7a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyperparams_from_excel(data):\n",
    "    results = []\n",
    "    for _, row in data.iterrows():\n",
    "        model = row['Model']  # Remplace par le nom exact de la colonne \"Modèle\"\n",
    "        dataset = row['Dataset']  # Remplace par le nom exact de la colonne \"Dataset\"\n",
    "        epochs = row['Epoch'] # Pour repérer la ligne où l'on stocker les meilleurs hyperparams\n",
    "        best_hyperparams = row['Parameters']  # Remplace par la colonne contenant {'lr': ..., 'latent_dim': ...}\n",
    "        \n",
    "        # Convertir les hyperparamètres en dictionnaire Python\n",
    "        if epochs == \"Best\":\n",
    "            hyperparams = ast.literal_eval(best_hyperparams)\n",
    "        \n",
    "            results.append({\n",
    "                'Modèle': model,\n",
    "                'Dataset': dataset,\n",
    "                'Hyperparamètres': hyperparams\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "colorization_path = '/home/rlaurie/Bureau/5A/HDDL/projet3/results_color.ods'\n",
    "data_color = pd.read_excel(colorization_path, engine=\"odf\")\n",
    "print(data_color.head())\n",
    "extracted_color = extract_hyperparams_from_excel(data_color)\n",
    "\n",
    "# Exemple d'utilisation des résultats\n",
    "for entry in extracted_color:\n",
    "    print(f\"Modèle : {entry['Modèle']}\")\n",
    "    print(f\"Dataset : {entry['Dataset']}\")\n",
    "    print(f\"Hyperparamètres : {entry['Hyperparamètres']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "inpainting_path = '/home/rlaurie/Bureau/5A/HDDL/projet3/results_inpainting.ods'\n",
    "data_inpaint = pd.read_excel(inpainting_path, engine=\"odf\")\n",
    "print(data_inpaint.head())\n",
    "extracted_inpaint = extract_hyperparams_from_excel(data_inpaint)\n",
    "\n",
    "# Exemple d'utilisation des résultats\n",
    "for entry in extracted_inpaint:\n",
    "    print(f\"Modèle : {entry['Modèle']}\")\n",
    "    print(f\"Dataset : {entry['Dataset']}\")\n",
    "    print(f\"Hyperparamètres : {entry['Hyperparamètres']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "mae_path = '/home/rlaurie/Bureau/5A/HDDL/projet3/results_mae.ods'\n",
    "data_mae = pd.read_excel(mae_path, engine=\"odf\")\n",
    "print(data_mae.head())\n",
    "extracted_mae = extract_hyperparams_from_excel(data_mae)\n",
    "\n",
    "# Exemple d'utilisation des résultats\n",
    "for entry in extracted_mae:\n",
    "    print(f\"Modèle : {entry['Modèle']}\")\n",
    "    print(f\"Dataset : {entry['Dataset']}\")\n",
    "    print(f\"Hyperparamètres : {entry['Hyperparamètres']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f14ef-12f6-4515-9dd8-f8f8bcc4017f",
   "metadata": {},
   "source": [
    "## Evaluation de nos résultats avec des courbes ROC et métrique AUROC \n",
    "\n",
    "Maintenant que nous avons nos hyperparamètres optimisés pour chaque combinaison de modèles et de datasets, nous pouvons passer à l'éévaluation de leur performance, avec des courbes ROC et métrique AUROC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646d9078-e767-497b-951c-2b5c9a5790cf",
   "metadata": {},
   "source": [
    "### Threshold (Seuil)\n",
    "\n",
    "Tout d'abord pour les courbes ROC, nous deons définir un seuil. En effet, ce seuil servira à classifier les sorties du modèles. En effet, on le construit de telle manière que 90% des images du jeu d'entraînement soient reconnues comment normale et on regarde si la précision de notre recostruction est au-dessus ou en-dessous de ce seuil pour définir s'il y a une anomalie ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0aa9d8-6958-4bdb-a843-c6c0aadd68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_thresholds_for_multiple_models(models, train_loader, criterion, device='cuda', normal_ratio=0.9):\n",
    "    \"\"\"\n",
    "    Calculer les seuils d'anomalie pour plusieurs modèles en utilisant l'ensemble d'entraînement.\n",
    "    Args:\n",
    "        models (dict): Dictionnaire des modèles avec leurs noms comme clés.\n",
    "        train_loader: DataLoader pour l'ensemble d'entraînement.\n",
    "        criterion: Fonction de perte pour calculer les scores d'anomalie.\n",
    "        device (str): Appareil à utiliser ('cuda' ou 'cpu').\n",
    "        normal_ratio (float): Ratio d'échantillons à classer comme \"normaux\" (par défaut 0.9, soit 90%).\n",
    "    Returns:\n",
    "        dict: Seuils calculés pour chaque modèle.\n",
    "    \"\"\"\n",
    "    thresholds = {}\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    \n",
    "        anomaly_scores = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for images, _ in train_loader:\n",
    "                images = images.to(device)\n",
    "                output, _ = model(images)\n",
    "                # Calculer la perte comme score d'anomalie\n",
    "                loss = torch.mean((output - images) ** 2, dim=(1, 2, 3))\n",
    "                anomaly_scores.extend(loss.cpu().numpy())\n",
    "    \n",
    "        # Conversion en array\n",
    "        anomaly_scores = np.array(anomaly_scores)\n",
    "    \n",
    "        # Calculer le seuil au 90e centile des scores d'anomalie\n",
    "        threshold = np.percentile(anomaly_scores, normal_ratio * 100)\n",
    "        thresholds[model_name] = threshold\n",
    "        print(f\"Threshold for {model_name} (90% normal): {threshold:.4f}\")\n",
    "    \n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9e83e-a995-49fd-b2cc-bf76f2c4c055",
   "metadata": {},
   "source": [
    "### ROC, AUROC, Matrice de Confusion et métrique de Précision, Recall et F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd010961-07e8-49be-8a17-e7a71833a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_anomaly_detection_with_threshold_multiple_models(models, test_loader, dataset_name, criterion, thresholds, device='cuda'):\n",
    "    \"\"\"\n",
    "    Évaluer la détection d'anomalies pour plusieurs modèles et tracer les courbes ROC sur le même graphique.\n",
    "    Args :\n",
    "        models (dict) : Dictionnaire de modèles avec leurs noms comme clés.\n",
    "        test_loader : DataLoader pour le jeu de données de test.\n",
    "        dataset_name (str) : Nom du jeu de données (pour le titre du graphique).\n",
    "        criterion : Fonction de perte pour calculer les scores d'anomalie.\n",
    "        thresholds (dict) : Dictionnaire des seuils pour chaque modèle.\n",
    "        device (str) : Périphérique à utiliser (« cuda » ou « cpu »).\n",
    "    Retourne :\n",
    "        avg_roc_auc : Moyenne de l'AUROC sur l'ensemble des modèles.\n",
    "    \"\"\"\n",
    "    roc_data = {}  # Stocker le FPR, le TPR et l'AUC pour chaque modèle\n",
    "    all_predictions = []\n",
    "    all_ground_truth = []\n",
    "    \n",
    "    # Evaluate each model\n",
    "    for model_name, model in models.items():\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        anomaly_scores = []\n",
    "        ground_truth = []\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                output, _ = model(images)\n",
    "                loss = torch.mean((output - images) ** 2, dim=(1, 2, 3))\n",
    "                anomaly_scores.extend(loss.cpu().numpy())\n",
    "                \n",
    "                # Tracer les étiquettes multi-classes en binaire (en supposant que 0 = bad, autre = good)\n",
    "                binary_labels = (labels != 1).int()  # 0 -> bad, 1 -> good\n",
    "                ground_truth.extend(binary_labels.cpu().numpy())\n",
    "\n",
    "        # Convertir en tableaux numpy\n",
    "        anomaly_scores = np.array(anomaly_scores)\n",
    "        ground_truth = np.array(ground_truth)\n",
    "\n",
    "        # Appliquer le seuil spécifique au modèle pour générer des prédictions\n",
    "        threshold = thresholds.get(model_name, 0.05)   # Valeur par défaut de 0,05 si le seuil n'est pas fourni\n",
    "        preds = (anomaly_scores > threshold).astype(int)  # 0 si anomalie, 1 si normal\n",
    "        predictions.extend(preds)\n",
    "\n",
    "        # Calcul de la courbe ROC et de l'AUROC\n",
    "        fpr, tpr, thresholds_roc = roc_curve(ground_truth, anomaly_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Stocker les résultats\n",
    "        roc_data[model_name] = {'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc}\n",
    "\n",
    "        # Calcul de la précision, du rappel et du score F1\n",
    "        precision = precision_score(ground_truth, preds)\n",
    "        recall = recall_score(ground_truth, preds)\n",
    "        f1 = f1_score(ground_truth, preds)\n",
    "\n",
    "        # Affichage de la matrice de confusion\n",
    "        cm = confusion_matrix(ground_truth, preds)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Anomalie', 'Normal'], yticklabels=['Anomalie', 'Normal'])\n",
    "        plt.title(f'Matrice de Confusion pour {model_name}')\n",
    "        plt.xlabel('Prédictions')\n",
    "        plt.ylabel('Véritables Labels')\n",
    "        plt.show()\n",
    "\n",
    "        # Affichage des métriques de précision, de rappel et de score F1\n",
    "        print(f\"{model_name} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "        print(f\"{model_name} - Confusion Matrix:\\n{cm}\\n\")\n",
    "\n",
    "                # Recueillir les prédictions et la vérité de terrain pour une évaluation globale\n",
    "        all_predictions.extend(preds)\n",
    "        all_ground_truth.extend(ground_truth)\n",
    "\n",
    "    # Calculer la courbe ROC moyenne\n",
    "    all_fpr = np.unique(np.concatenate([roc_data[m]['fpr'] for m in roc_data]))\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "    for model_name in roc_data:\n",
    "        mean_tpr += np.interp(all_fpr, roc_data[model_name]['fpr'], roc_data[model_name]['tpr'])\n",
    "\n",
    "    mean_tpr /= len(models)\n",
    "    mean_roc_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "    # Tracer toutes les courbes ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for model_name in roc_data:\n",
    "        plt.plot(\n",
    "            roc_data[model_name]['fpr'], \n",
    "            roc_data[model_name]['tpr'], \n",
    "            lw=2, label=f\"{model_name} (AUC = {roc_data[model_name]['roc_auc']:.2f})\"\n",
    "        )\n",
    "\n",
    "    # Tracer la courbe ROC moyenne\n",
    "    plt.plot(\n",
    "        all_fpr, \n",
    "        mean_tpr, \n",
    "        color='violet', \n",
    "        lw=2, linestyle='-', \n",
    "        label=f\"Mean ROC (AUC = {mean_roc_auc:.2f})\"\n",
    "    )\n",
    "\n",
    "    # Ligne diagonale pour référence\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'ROC Curves for Multiple Models on {dataset_name}', fontsize=14)\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return mean_roc_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76bb58-1405-465d-ade3-bf0a9268d682",
   "metadata": {},
   "source": [
    "### Fonction pour tout exécuter en 1 fois\n",
    "\n",
    "Comme chaque modèle doit être entrainé sur chaque dataset, cela peut arriver de se perdre et d'entraîner le modèle sur un mauvais dataset, alors nous avons tous rassemblé en une fonction qui prend en entrée les DataLoaders et les hyperparamètes associés à ce dataset et qui ressort les courbes ROC et le score AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba889bd-e36a-4495-81e2-27a5eabef6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_threshold_roc_curves_auroc_confusion(train_loader, test_loader, dataset_name, device, \n",
    "                                         epochs=20, \n",
    "                                         models_hyperparams=None,\n",
    "                                         save_dir='/home/rlaurie/Bureau/5A/HDDL/projet3/poids'):\n",
    "    \"\"\"\n",
    "    Entraînement des modèles, calcul des seuils, évaluation des courbes ROC et des scores AUROC, affichage des matrices de confusions et calcul des métrqiues de Précision de Recall et de score F1\n",
    "    \n",
    "    Args :\n",
    "        train_loader : DataLoader pour l'ensemble d'entraînement.\n",
    "        test_loader : DataLoader pour l'ensemble de tests.\n",
    "        dataset_name : Nom de l'ensemble de données pour le tracé.\n",
    "        epochs : Nombre d'époques d'apprentissage.\n",
    "        models_hyperparams : Dictionnaire contenant des hyperparamètres spécifiques au modèle, tels que \n",
    "                             latent_dim, lr et mask_size.\n",
    "                             Exemple de format :\n",
    "                             {\n",
    "                                Inpainting » : {“latent_dim” : 128, “mask_size” : 36, 'lr' : 0.001},\n",
    "                                Colorization' : {'latent_dim' : 128, 'lr' : 0.001},\n",
    "                                MaskedAutoencoder' : {'latent_dim' : 128, 'mask_ratio' : 0.125, 'lr' : 0.001}\n",
    "                             }\n",
    "        save_dir : Répertoire où les modèles seront sauvegardés.\n",
    "    \"\"\"\n",
    "    print(device)\n",
    "    \n",
    "    # Initialiser les modèles avec des hyperparamètres spécifiques\n",
    "    inpainting_model = InpaintingModel(mask_size=models_hyperparams['Inpainting']['mask_size'])\n",
    "    colorization_model = ColorizationModel(latent_dim=models_hyperparams['Colorization']['latent_dim'])\n",
    "    mae_model = MaskedAutoencoderModel(latent_dim=models_hyperparams['MaskedAutoencoder']['latent_dim'], \n",
    "                                       mask_ratio=models_hyperparams['MaskedAutoencoder']['mask_ratio'])\n",
    "\n",
    "    print(\"####  Inpainting Model Training  #####\")\n",
    "    inpainting_encoder, val_loss, inpaintin_model = train_ssl_model(inpainting_model,\n",
    "                                         train_loader, \n",
    "                                         test_loader, \n",
    "                                         criterion=nn.MSELoss(), \n",
    "                                         optimizer=optim.Adam(inpainting_model.parameters(), \n",
    "                                                              lr=models_hyperparams['Inpainting']['lr']),\n",
    "                                         device=device,\n",
    "                                         epochs=epochs,\n",
    "                                         show_plot=True)\n",
    "\n",
    "    print(\"####  Colorization Model Training  #####\")\n",
    "    colorization_encoder, val_loss, colorization_model = train_ssl_model(colorization_model, \n",
    "                                           train_loader, \n",
    "                                           test_loader, \n",
    "                                           criterion=nn.MSELoss(), \n",
    "                                           optimizer=optim.Adam(colorization_model.parameters(), \n",
    "                                                                lr=models_hyperparams['Colorization']['lr']),\n",
    "                                           device=device,\n",
    "                                           epochs=epochs,\n",
    "                                           show_plot=True)\n",
    "\n",
    "    print(\"####  Masked AutoEncoder Model Training  #####\")\n",
    "    mae_encoder, val_loss, mae_model = train_ssl_model(mae_model,\n",
    "                                  train_loader, \n",
    "                                  test_loader, \n",
    "                                  criterion=nn.MSELoss(), \n",
    "                                  optimizer=optim.Adam(mae_model.parameters(), \n",
    "                                                       lr=models_hyperparams['MaskedAutoencoder']['lr']),\n",
    "                                  device=device,\n",
    "                                  epochs=epochs,\n",
    "                                  show_plot=True)\n",
    "\n",
    "    # Pour s'assurer que le répertoire de sauvegarde existe\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Sauvegarder les modèles\n",
    "    print(\"####  Saving Models  #####\")\n",
    "    torch.save(inpainting_model.state_dict(), f\"{save_dir}/{dataset_name}_inpainting_model.pth\")\n",
    "    torch.save(colorization_model.state_dict(), f\"{save_dir}/{dataset_name}_colorization_model.pth\")\n",
    "    torch.save(mae_model.state_dict(), f\"{save_dir}/{dataset_name}_mae_model.pth\")\n",
    "\n",
    "    models = {\n",
    "        'Inpainting': inpainting_model,\n",
    "        'Colorization': colorization_model,\n",
    "        'Masked Autoencoder': mae_model\n",
    "    }\n",
    "\n",
    "    print(\"####  Thresholds computations  #####\")\n",
    "    # Calcul des seuils pour tous les modèles\n",
    "    thresholds = compute_thresholds_for_multiple_models(\n",
    "        models=models, \n",
    "        train_loader=train_loader, \n",
    "        criterion=nn.MSELoss(), \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Évaluer et tracer les courbes ROC mais aussi affichage de la matrice de confusion et des métriques de Précision, Recall et F1 Score\n",
    "    mean_roc_auc = evaluate_anomaly_detection_with_threshold_multiple_models(\n",
    "        models=models, \n",
    "        test_loader=test_loader,\n",
    "        dataset_name=dataset_name,\n",
    "        criterion=nn.MSELoss(), \n",
    "        thresholds=thresholds, \n",
    "        device=device\n",
    "    )\n",
    "    return inpainting_model, colorization_model, mae_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de5b28-ed77-49b2-b6a1-6736a69b75c5",
   "metadata": {},
   "source": [
    "### Hazelnut Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a57ef10-7ab4-4d4f-bbfd-f1c81facb78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer pour \"hazelnut\" et reformater\n",
    "hazelnut_mae = next((entry for entry in extracted_mae if entry['Dataset'] == 'hazelnut'), None)\n",
    "formatted_hazelnut_mae = {hazelnut_mae['Modèle']: hazelnut_mae['Hyperparamètres']}\n",
    "print(formatted_hazelnut_mae)\n",
    "\n",
    "hazelnut_color = next((entry for entry in extracted_color if entry['Dataset'] == 'hazelnut'), None)\n",
    "formatted_hazelnut_color = {hazelnut_color['Modèle']: hazelnut_color['Hyperparamètres']}\n",
    "print(formatted_hazelnut_color)\n",
    "\n",
    "hazelnut_inpaint = next((entry for entry in extracted_inpaint if entry['Dataset'] == 'hazelnut'), None)\n",
    "formatted_hazelnut_inpaint= {hazelnut_inpaint['Modèle']: hazelnut_inpaint['Hyperparamètres']}\n",
    "print(formatted_hazelnut_inpaint)\n",
    "\n",
    "models_hyperparams_hazelnut = {**formatted_hazelnut_inpaint, **formatted_hazelnut_color, **formatted_hazelnut_mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d50b97-e8de-4be6-b614-9d5fee3d589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainting_model, colorization_model, mae_model = training_threshold_roc_curves_auroc_confusion(train_loader = hazelnut_train_loader, \n",
    "                                                                                                test_loader = hazelnut_test_loader,\n",
    "                                                                                                dataset_name = \"Hazelnut Dataset\", \n",
    "                                                                                                device='cuda', \n",
    "                                                                                                epochs=40, \n",
    "                                                                                                models_hyperparams=models_hyperparams_hazelnut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70254f8-d014-49cc-a959-0d7220028bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la colorisation sur des images test aléatoires\n",
    "visualize_reconstructions(inpainting_model, hazelnut_test_loader, device)\n",
    "visualize_reconstructions(colorization_model, hazelnut_test_loader, device)\n",
    "visualize_reconstructions(mae_model, hazelnut_test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5e005-f97d-4d84-ab04-fe8ff9bf1a5b",
   "metadata": {},
   "source": [
    "### Bottle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88a794-5d75-427a-9979-aac50f9e0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer pour \"bottle\" et reformater\n",
    "bottle_mae = next((entry for entry in extracted_mae if entry['Dataset'] == 'bottle'), None)\n",
    "formatted_bottle_mae = {bottle_mae['Modèle']: bottle_mae['Hyperparamètres']}\n",
    "print(formatted_bottle_mae)\n",
    "\n",
    "bottle_color = next((entry for entry in extracted_color if entry['Dataset'] == 'bottle'), None)\n",
    "formatted_bottle_color = {bottle_color['Modèle']: bottle_color['Hyperparamètres']}\n",
    "print(formatted_bottle_color)\n",
    "\n",
    "bottle_inpaint = next((entry for entry in extracted_inpaint if entry['Dataset'] == 'bottle'), None)\n",
    "formatted_bottle_inpaint= {bottle_inpaint['Modèle']: bottle_inpaint['Hyperparamètres']}\n",
    "print(formatted_bottle_inpaint)\n",
    "\n",
    "models_hyperparams_bottle = {**formatted_bottle_inpaint, **formatted_bottle_color, **formatted_bottle_mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90a3f1-44a1-46c0-a592-81a10f530fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainting_model, colorization_model, mae_model = training_threshold_roc_curves_auroc_confusion(train_loader = bottle_train_loader, \n",
    "                                                                                                test_loader = bottle_test_loader,\n",
    "                                                                                                dataset_name = \"Bottle Dataset\", \n",
    "                                                                                                device='cuda', \n",
    "                                                                                                epochs=40, \n",
    "                                                                                                models_hyperparams=models_hyperparams_bottle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dcc658-7806-4be2-b18c-7904a4b8d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la colorisation sur des images test aléatoires\n",
    "visualize_reconstructions(inpainting_model, bottle_test_loader, device)\n",
    "visualize_reconstructions(colorization_model, bottle_test_loader, device)\n",
    "visualize_reconstructions(mae_model, bottle_test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce55ff0c-0829-422a-91a7-eddf3ff89d71",
   "metadata": {},
   "source": [
    "Enlevé pour mettre le fichier sur le git..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089849f9-aa63-485e-b725-7192835e62bd",
   "metadata": {},
   "source": [
    "### Capsule Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67118199-f5b0-4676-b6b6-caa1ccabc537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer pour \"capsule\" et reformater\n",
    "capsule_mae = next((entry for entry in extracted_mae if entry['Dataset'] == 'capsule'), None)\n",
    "formatted_capsule_mae = {capsule_mae['Modèle']: capsule_mae['Hyperparamètres']}\n",
    "#print(formatted_capsule_mae)\n",
    "\n",
    "capsule_color = next((entry for entry in extracted_color if entry['Dataset'] == 'capsule'), None)\n",
    "formatted_capsule_color = {capsule_color['Modèle']: capsule_color['Hyperparamètres']}\n",
    "#print(formatted_capsule_color)\n",
    "\n",
    "capsule_inpaint = next((entry for entry in extracted_inpaint if entry['Dataset'] == 'capsule'), None)\n",
    "formatted_capsule_inpaint= {capsule_inpaint['Modèle']: capsule_inpaint['Hyperparamètres']}\n",
    "#print(formatted_capsule_inpaint)\n",
    "\n",
    "models_hyperparams_capsule = {**formatted_capsule_inpaint, **formatted_capsule_color, **formatted_capsule_mae}\n",
    "print(models_hyperparams_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5903ddc9-9432-474c-aa8c-8ffa7a3a87c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainting_model, colorization_model, mae_model = training_threshold_roc_curves_auroc_confusion(train_loader = capsule_train_loader, \n",
    "                                                                                                test_loader = capsule_test_loader,\n",
    "                                                                                                dataset_name = \"Capsule Dataset\", \n",
    "                                                                                                device='cuda', \n",
    "                                                                                                epochs=40, \n",
    "                                                                                                models_hyperparams=models_hyperparams_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711d103-ca1c-450d-9adb-cccb2793b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la colorisation sur des images test aléatoires\n",
    "visualize_reconstructions(inpainting_model, capsule_test_loader, device)\n",
    "visualize_reconstructions(colorization_model, capsule_test_loader, device)\n",
    "visualize_reconstructions(mae_model, capsule_test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff57ec76-b52f-4d39-9822-5850379fc41c",
   "metadata": {},
   "source": [
    "### Toothbrush Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745f61d-dc3f-4214-9289-e04d7b10cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer pour \"toothbrush\" et reformater\n",
    "toothbrush_mae = next((entry for entry in extracted_mae if entry['Dataset'] == 'toothbrush'), None)\n",
    "formatted_toothbrush_mae = {toothbrush_mae['Modèle']: toothbrush_mae['Hyperparamètres']}\n",
    "#print(formatted_toothbrush_mae)\n",
    "\n",
    "toothbrush_color = next((entry for entry in extracted_color if entry['Dataset'] == 'toothbrush'), None)\n",
    "formatted_toothbrush_color = {toothbrush_color['Modèle']: toothbrush_color['Hyperparamètres']}\n",
    "#print(formatted_toothbrush_color)\n",
    "\n",
    "toothbrush_inpaint = next((entry for entry in extracted_inpaint if entry['Dataset'] == 'toothbrush'), None)\n",
    "formatted_toothbrush_inpaint= {toothbrush_inpaint['Modèle']: toothbrush_inpaint['Hyperparamètres']}\n",
    "#print(formatted_toothbrush_inpaint)\n",
    "\n",
    "models_hyperparams_toothbrush = {**formatted_toothbrush_inpaint, **formatted_toothbrush_color, **formatted_toothbrush_mae}\n",
    "print(models_hyperparams_toothbrush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6e34f2-e2ea-4fcd-8c4e-9747e57cfeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainting_model, colorization_model, mae_model = training_threshold_roc_curves_auroc_confusion(train_loader = toothbrush_train_loader, \n",
    "                                                                                                test_loader = toothbrush_test_loader,\n",
    "                                                                                                dataset_name = \"Toothbrush Dataset\", \n",
    "                                                                                                device='cuda', \n",
    "                                                                                                epochs=40, \n",
    "                                                                                                models_hyperparams=models_hyperparams_toothbrush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b5d72-44f7-4133-9269-ededd2980bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la colorisation sur des images test aléatoires\n",
    "visualize_reconstructions(inpainting_model, toothbrush_test_loader, device)\n",
    "visualize_reconstructions(colorization_model, toothbrush_test_loader, device)\n",
    "visualize_reconstructions(mae_model, toothbrush_test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63db174e-08cf-4fb2-b98b-fd60ed6abfc5",
   "metadata": {},
   "source": [
    "### Engine Wiring Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f18d1-237d-4605-b896-38422dcf0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer pour \"engine_wiring\" et reformater\n",
    "engine_wiring_mae = next((entry for entry in extracted_mae if entry['Dataset'] == 'engine_wiring'), None)\n",
    "formatted_engine_wiring_mae = {engine_wiring_mae['Modèle']: engine_wiring_mae['Hyperparamètres']}\n",
    "#print(formatted_engine_wiring_mae)\n",
    "\n",
    "engine_wiring_color = next((entry for entry in extracted_color if entry['Dataset'] == 'engine_wiring'), None)\n",
    "formatted_engine_wiring_color = {engine_wiring_color['Modèle']: engine_wiring_color['Hyperparamètres']}\n",
    "#print(formatted_engine_wiring_color)\n",
    "\n",
    "engine_wiring_inpaint = next((entry for entry in extracted_inpaint if entry['Dataset'] == 'engine_wiring'), None)\n",
    "formatted_engine_wiring_inpaint = {engine_wiring_inpaint['Modèle']: engine_wiring_inpaint['Hyperparamètres']}\n",
    "#print(formatted_engine_wiring_inpaint)\n",
    "\n",
    "models_hyperparams_engine_wiring = {**formatted_engine_wiring_inpaint, **formatted_engine_wiring_color, **formatted_engine_wiring_mae}\n",
    "print(models_hyperparams_engine_wiring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf713633-ef2e-4eac-8e19-10f91c767595",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpainting_model, colorization_model, mae_model = training_threshold_roc_curves_auroc_confusion(train_loader = engine_wiring_train_loader, \n",
    "                                                                                                test_loader = engine_wiring_test_loader,\n",
    "                                                                                                dataset_name = \"Engine Wiring Dataset\", \n",
    "                                                                                                device='cuda', \n",
    "                                                                                                epochs=40, \n",
    "                                                                                                models_hyperparams=models_hyperparams_engine_wiring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc32811-67d6-43c3-87f0-7a8b1be970d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser la colorisation sur des images test aléatoires\n",
    "visualize_reconstructions(inpainting_model, engine_wiring_test_loader, device)\n",
    "visualize_reconstructions(colorization_model, engine_wiring_test_loader, device)\n",
    "visualize_reconstructions(mae_model, engine_wiring_test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4ecbf-de46-4182-a6d5-cdf5a638e026",
   "metadata": {},
   "source": [
    "## Améliorations Futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28acc04-0f0b-476d-b352-b002f79f1b66",
   "metadata": {},
   "source": [
    "1. **Exploration de modèles alternatifs**\n",
    "\n",
    "D'autres architectures pour a détections d'anomalies pourraient être explorées pour essayer capturer des relations globales plus efficacement en fonction de l'anomalie.\n",
    "\n",
    "2. **Optimisation des hyperparamètres**\n",
    "\n",
    "Avec plus de temps et une plus grande puissance, on aurait pu tester nos hyperparamètres sur des plages encore plus grandes lors de notre validation croisée.\n",
    "\n",
    "3. **Test sur les différents types d'anomalies et entraînement plus long**\n",
    "\n",
    "Avec plus de temps, plus d'époques pour certains modèles auraient probablement pu améliorer nos résultats et nos reconstructions encore plus. \n",
    "Aussi, nous aurions pu essayer de voir si certaines anomalies étaient mieux détectées que d'autres et essayer de fine-tunner nos modèles encore plus pour essayer de capter ces anomalies qui auraient pu passer à travers nos modèles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
